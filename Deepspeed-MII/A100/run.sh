python pipeline.py --model "meta-llama/Llama-2-7b-hf" --max-new-tokens=128 --max_input_length=16 --batch_size=1
python pipeline.py --model "meta-llama/Llama-2-7b-hf" --max-new-tokens=128 --max_input_length=1024 --batch_size=1
python pipeline.py --model "meta-llama/Llama-2-7b-hf" --max-new-tokens=128 --max_input_length=16 --batch_size=128
python pipeline.py --model "meta-llama/Llama-2-7b-hf" --max-new-tokens=128 --max_input_length=512 --batch_size=128